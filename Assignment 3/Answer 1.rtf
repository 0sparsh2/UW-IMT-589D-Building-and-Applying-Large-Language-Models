{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs29\fsmilli14667 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Question 1
\f1\fs24 \

\f0\fs29\fsmilli14667 Describe in your own words (one-two paragraphs, half a page max) what problems an LSTM architecture addresses and how. [3 points]
\f1\fs24 \
\

\f0\fs29\fsmilli14667 Answer 1
\f1\fs24 \
\pard\pardeftab720\qj\partightenfactor0

\f0\fs29\fsmilli14667 \cf0 Long Short-Term Memory (LSTM) architectures were developed to tackle fundamental issues encountered in traditional recurrent neural networks (RNNs), particularly regarding the learning of long-range dependencies and the problem of vanishing or exploding gradients during training. In standard RNNs, the vanishing gradient problem arises when gradients diminish exponentially as they propagate back through time, making it difficult for the model to effectively learn dependencies between distant time steps in sequential data.
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\qj\partightenfactor0

\f0\fs29\fsmilli14667 \cf0 LSTMs address these challenges by introducing a more sophisticated memory cell structure equipped with gating mechanisms. These gated cells, composed of input, forget, and output gates, regulate the flow of information within the network. The input gate determines which values should be updated, the forget gate decides which information to discard from the cell state, and the output gate controls the information that will be used to make predictions or propagate to subsequent time steps. This architecture enables LSTMs to selectively retain important information over extended sequences while mitigating the vanishing gradient problem, thereby facilitating the learning of long-term dependencies.
\f1\fs24 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\qj\partightenfactor0

\f0\fs29\fsmilli14667 \cf0 The ability of LSTMs to learn and retain information over multiple time steps makes them well-suited for tasks involving sequential data, such as natural language processing, speech recognition, and time series forecasting. By addressing the limitations of traditional RNNs, LSTMs have become a cornerstone in deep learning applications that require modeling complex dependencies across temporal sequences.
\f1\fs24 \
}